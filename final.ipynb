{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b388891",
   "metadata": {},
   "source": [
    "# CIS4930 -- Final Project\n",
    "## Developed by: Chloe Fandino (Team Leader), Ashley James, Madelyne Wirbel, Chloe Nolan, Christopher Enlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961bb43",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81630e6b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3080896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports here :)\n",
    "\n",
    "# TODO: DELETE ---> any imports that don't end up getting used by the end of the project !!!!\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3635f",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity.csv') # import the data from the csv file --> convert to df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4edfe",
   "metadata": {},
   "source": [
    "### Exploration of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 100) # for purposes of looking at data --> need to see all rows\n",
    "\n",
    "# basic intitial looks at the dataset\n",
    "print(df.shape)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace from column names\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27bb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist()) # print out all of the available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # null values? --> NONE :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25398568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() # duplicate values? --> NONE :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539006c-7e46-4a25-8098-44037eeeaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any infinities exist in the dataframe\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "has_inf = np.isinf(numeric_df.to_numpy()).any()\n",
    "\n",
    "print(has_inf) # will need to handle in cleaning\n",
    "inf_cols = numeric_df.columns[np.isinf(numeric_df.to_numpy()).any(axis=0)].tolist()\n",
    "print(\"Columns with inf:\", inf_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a799c4bc",
   "metadata": {},
   "source": [
    "#### Visualization of the target variable --> shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of shares\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df[\"shares\"], bins=50)\n",
    "plt.title(\"Distribution of Shares (Raw Scale)\")\n",
    "plt.xlabel(\"Shares\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.yscale(\"log\")  # long tail\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of raw shares --> view of the outliers\n",
    "sns.boxplot(x = df[\"shares\"])\n",
    "plt.title(\"Boxplot of Shares (Visualization of Outliers)\")\n",
    "plt.ylabel(\"Shares\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9071771",
   "metadata": {},
   "source": [
    "#### Visualizations of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63373d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of key numerical features\n",
    "key_cols = [\"n_tokens_title\", \"n_tokens_content\", \"num_imgs\", \"num_hrefs\"]\n",
    "\n",
    "for col in key_cols:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(df[col], bins=50)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8bb7e3",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34de976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # exploration of potentially non-predictive features\n",
    "df = df.drop(columns=['url', 'timedelta'], errors='ignore') # url is a title and number of days since posted until added to the dataset --> no predictive qualities\n",
    "\n",
    "# feature engineering\n",
    "df['rate_positive_words'] = df['global_rate_positive_words'] / (df['n_tokens_content'] + 1)\n",
    "df['rate_negative_words'] = df['global_rate_negative_words'] / (df['n_tokens_content'] + 1)\n",
    "df['emotional_polarity'] = df['global_sentiment_polarity'].abs()\n",
    "df['title_body_sentiment_ratio'] = df['title_sentiment_polarity'] / (df['global_sentiment_polarity'] + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHLOE target creation\n",
    "median_shares = df['shares'].median()\n",
    "print(f\"Splitting data at median shares: {median_shares}\")\n",
    "\n",
    "def categorize(x):\n",
    "    return 1 if x > median_shares else 0\n",
    "\n",
    "df['y'] = df['shares'].apply(categorize)\n",
    "\n",
    "# define X and y \n",
    "X = df.drop(columns=['shares', 'y'])\n",
    "y = df['y']\n",
    "\n",
    "binary_cols = [col for col in X.columns if \"data_channel\" in col or \"weekday\" in col or \"is_weekend\" in col]\n",
    "# ensure binary cols are actually integers\n",
    "for col in binary_cols:\n",
    "    X[col] = X[col].astype(int)\n",
    "\n",
    "numeric_cols = [col for col in X.columns if col not in binary_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify binary columns and ensure they are ints\n",
    "# binary_cols = [col for col in df.columns \n",
    "#                if col.startswith(\"data_channel_is_\") or col.startswith(\"weekday_is_\")]\n",
    "\n",
    "# # ensure binary indicator columns are integers\n",
    "# df.columns = df.columns.str.strip()\n",
    "# print(\"Binary indicator columns:\", binary_cols)\n",
    "\n",
    "# for col in binary_cols:\n",
    "#     df[col] = df[col].astype(int)\n",
    "\n",
    "# df[binary_cols].dtypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> test-train-split <-- DO NOT EDIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHLOE pearson correlation coefficient\n",
    "corr_matrix = X_train[numeric_cols].corr(method='pearson').abs()\n",
    "\n",
    "# visualization of highly correlated features\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    cmap='coolwarm',\n",
    "    annot=False,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    "    square=True\n",
    ")\n",
    "plt.title(\"Pearson Correlation Heatmap of Numeric Features (absolute value)\")\n",
    "plt.show()\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "\n",
    "print(f\"Dropping {len(to_drop)} columns due to correlation: {to_drop}\")\n",
    "\n",
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)\n",
    "numeric_cols = [c for c in numeric_cols if c not in to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f9d86",
   "metadata": {},
   "source": [
    "#### Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize and count anomalies\n",
    "def anomaly_detection(feature, visualize):\n",
    "    # first boxplot to see potential outliers\n",
    "    if visualize:\n",
    "        sns.boxplot(x = df[feature], color = 'purple')\n",
    "        plt.title(feature)\n",
    "        plt.show()\n",
    "\n",
    "    # second calculate outliers based on IQR\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "\n",
    "    anomalies = df[(df[feature] < lower) | (df[feature] > upper)]\n",
    "    print('Anomalies: \\n', anomalies) # prints a list of potential anomalies\n",
    "\n",
    "    num_anomalies = anomalies.shape[0]\n",
    "    return num_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78744fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE OUTLIER HANDLING --> visualize outliers\n",
    "cols_remaining = df.columns.tolist() # what columns are left in the dataset\n",
    "\n",
    "num_anomalies_1 = []\n",
    "\n",
    "# for each of the remaining columns print the anomalies and see if there needs to be any adjustments made --> generally high rates of anomalies\n",
    "for col in cols_remaining:\n",
    "    num = anomaly_detection(col, True)\n",
    "    num_anomalies_1.append(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1b0da",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad42246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHLOE handling skew and outliers\n",
    "scaler = RobustScaler()\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfa28d",
   "metadata": {},
   "source": [
    "#### Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402bb39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHLOE TRAIN SMOTE NAN\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "#CLASS BALANCING (SMOTE)\n",
    "cat_indices = [X_train.columns.get_loc(c) for c in binary_cols if c in X_train.columns]\n",
    "print(\"Applying SMOTE...\")\n",
    "sm = SMOTENC(categorical_features=cat_indices, random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccfad7",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHLOE FEATURE SELECTION\n",
    "cat_indices = [X_train.columns.get_loc(c) for c in binary_cols if c in X_train.columns]\n",
    "print(\"Running Fast Feature Selection...\")\n",
    "selector_model = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)\n",
    "selector_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "selection = SelectFromModel(selector_model, threshold=\"1.25*median\", prefit=True)\n",
    "X_train_selected = selection.transform(X_train_res)\n",
    "X_test_selected = selection.transform(X_test)\n",
    "\n",
    "selected_features_names = X_train_res.columns[selection.get_support()]\n",
    "print(f\"Selected {len(selected_features_names)} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc7d16",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost...\")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    objective='binary:logistic'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_selected, y_train_res)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test_selected)\n",
    "\n",
    "#evaluate\n",
    "# TODO: THIS IS THE ONLY MODEL THAT USES THE ORDER NOT VIRAL, VIRAL --> weird just check it out\n",
    "labels = [\"Not Viral\", \"Viral\"] \n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"XGBoost Binary Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931158e7-7502-4ae6-ba92-7cec30a8d848",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2e11f",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fcecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to declare the variable once\n",
    "labels = ['Viral', 'Not Viral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2af1f3-e41b-4e13-a1d5-1f5a8b567a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a logistic regression model --> basic model for classification\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train)\n",
    "y_predict_log = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c46aa-f004-49fc-92c0-0823c5598543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate linear regression performance\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_predict_log, target_names = labels))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict_log)\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Purples', xticklabels = labels, yticklabels = labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHLOE RANDOM FOREST\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=20, \n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_predict_rf = rf_model.predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"\\nRandom Forest Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_predict_rf, target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict_rf)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.show()\n",
    "#feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_df = feature_df.sort_values(by='Importance', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc3ae9d",
   "metadata": {},
   "source": [
    "#### 2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor  = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "y_test_tensor  = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: investigate the architecture of the NN --> increase the complexity to better learn the pattern of the data\n",
    "class ClassificationNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 51),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(51, 26), # hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(26, num_classes)  # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationNN(input_dim=X_train_tensor.shape[1], num_classes=3)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Test loss\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            running_test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "#evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    y_pred = torch.argmax(outputs, dim=1)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test_tensor.numpy(), y_pred.numpy(), target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor.numpy(), y_pred.numpy())\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Training vs Test Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIS4930-ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
