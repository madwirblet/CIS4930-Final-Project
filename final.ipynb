{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b388891",
   "metadata": {},
   "source": [
    "# CIS4930 -- Final Project\n",
    "## Developed by: Chloe Fandino (Team Leader), Ashley James, Madelyne Wirbel, Chloe Nolan, Christopher Enlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961bb43",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81630e6b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3080896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports here :)\n",
    "\n",
    "# TODO: DELETE ---> any imports that don't end up getting used by the end of the project !!!!\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3635f",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity.csv') # import the data from the csv file --> convert to df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4edfe",
   "metadata": {},
   "source": [
    "### Exploration of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 100) # for purposes of looking at data --> need to see all rows\n",
    "\n",
    "# basic intitial looks at the dataset\n",
    "print(df.shape)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace from column names\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27bb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist()) # print out all of the available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # null values? --> NONE :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25398568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() # duplicate values? --> NONE :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539006c-7e46-4a25-8098-44037eeeaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any infinities exist in the dataframe\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "has_inf = np.isinf(numeric_df.to_numpy()).any()\n",
    "\n",
    "print(has_inf) # will need to handle in cleaning\n",
    "inf_cols = numeric_df.columns[np.isinf(numeric_df.to_numpy()).any(axis=0)].tolist()\n",
    "print(\"Columns with inf:\", inf_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8bb7e3",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34de976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploration of potentially non-predictive features\n",
    "df['url'].nunique() == len(df) # each example has a different url --> not needed in the dataset\n",
    "df = df.drop(columns = ['url'])\n",
    "\n",
    "df = df.drop(columns = ['timedelta']) # number of days since posted until added to the dataset --> no predictive qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify binary columns and ensure they are ints\n",
    "binary_cols = [col for col in df.columns \n",
    "               if col.startswith(\"data_channel_is_\") or col.startswith(\"weekday_is_\")]\n",
    "\n",
    "# ensure binary indicator columns are integers\n",
    "df.columns = df.columns.str.strip()\n",
    "print(\"Binary indicator columns:\", binary_cols)\n",
    "\n",
    "for col in binary_cols:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "df[binary_cols].dtypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling inf\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()\n",
    "print(\"Dataset shape after removing inf/NaN:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f6d54",
   "metadata": {},
   "source": [
    "#### Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of feature correlation --> highly correlated features do not need to both be in dataset\n",
    "df_correlation = df.corr()\n",
    "\n",
    "sns.heatmap(df_correlation, cmap = 'coolwarm', center = 0)\n",
    "plt.title(\"Correlation Heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dafc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr = df.drop(columns=['shares'])\n",
    "y_corr = df['shares']\n",
    "\n",
    "def drop_correlated_by_importance(X, y, threshold=0.7, n_estimators=50, protected_cols=[]):\n",
    "    model = ExtraTreesRegressor(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "    model.fit(X, y)\n",
    "    importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    corr_pairs = upper.stack().reset_index()\n",
    "    corr_pairs.columns = ['feat1', 'feat2', 'corr']\n",
    "    corr_pairs = corr_pairs[corr_pairs['corr'] > threshold]\n",
    "\n",
    "    to_drop = set()\n",
    "    for _, row in corr_pairs.iterrows():\n",
    "        if row['feat1'] in protected_cols or row['feat2'] in protected_cols: # we dont want to drop binary indicator cols\n",
    "            continue\n",
    "        if importances[row['feat1']] >= importances[row['feat2']]:\n",
    "            to_drop.add(row['feat2'])\n",
    "        else:\n",
    "            to_drop.add(row['feat1'])\n",
    "    return list(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = drop_correlated_by_importance(\n",
    "    X_corr, y_corr, threshold=0.80, n_estimators=50, protected_cols=binary_cols\n",
    ")\n",
    "\n",
    "print(\"Columns to drop due to high correlation:\", columns_to_drop)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# print new shape of dataset\n",
    "print(\"Dataset shape after dropping correlated features:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbdfdb",
   "metadata": {},
   "source": [
    "#### Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function with anomaly detection process --> first visualize, then calculate IQR\n",
    "\n",
    "## Currently not used in the final cleaning process, but kept for potential future use ##\n",
    "\n",
    "def anomaly_detection(feature):\n",
    "    # first boxplot to see potential outliers\n",
    "    sns.boxplot(x = df[feature], color = 'purple')\n",
    "    plt.title(feature)\n",
    "    plt.show()\n",
    "\n",
    "    # second calculate outliers based on IQR\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "\n",
    "    anomalies = df[(df[feature] < lower) | (df[feature] > upper)]\n",
    "    print('Anomalies: \\n', anomalies) # prints a list of potential anomalies\n",
    "\n",
    "    num_anomalies = anomalies.shape[0]\n",
    "    return num_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_remaining = df.columns.tolist() # what columns are left in the dataset\n",
    "\n",
    "print(cols_remaining)\n",
    "\n",
    "num_anomalies_1 = []\n",
    "\n",
    "# for each of the remaining columns print the anomalies and see if there needs to be any adjustments made --> generally high rates of anomalies\n",
    "for col in cols_remaining:\n",
    "    num = anomaly_detection(col)\n",
    "    num_anomalies_1.append(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae8749",
   "metadata": {},
   "source": [
    "#### Log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [c for c in df.columns if c not in binary_cols + ['shares', 'shares_cat']]\n",
    "\n",
    "skew_vals = df[numeric_cols].skew()\n",
    "\n",
    "skew_threshold = 1.0\n",
    "skewed_cols = skew_vals[skew_vals > skew_threshold].index.tolist()\n",
    "print(\"Skewed columns:\", skewed_cols)\n",
    "\n",
    "# updated log transform to ensure safe handling\n",
    "for col in skewed_cols:\n",
    "    col_min = df[col].min()\n",
    "    \n",
    "    if col_min <= -1:\n",
    "        df[col] = np.log1p(df[col] - col_min + 1.001)\n",
    "    else:\n",
    "        df[col] = np.log1p(df[col])\n",
    "\n",
    "# visualization of transformation\n",
    "for col in skewed_cols:\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    sns.histplot(df[col], bins = 50, ax = ax[0])\n",
    "    ax[0].set_title(f\"Original {col}\")\n",
    "    \n",
    "    sns.histplot(np.log1p(df[col]), bins = 50, ax = ax[1])\n",
    "    ax[1].set_title(f\"Log-transformed {col}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e5c2f",
   "metadata": {},
   "source": [
    "#### Windsorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric non-binary indicator columns\n",
    "numeric_cols = [c for c in df.columns if c not in binary_cols + ['shares']]\n",
    "\n",
    "# Winsorize\n",
    "for col in numeric_cols:\n",
    "    print(f\"Before winsorizing {col}: min={df[col].min()}, max={df[col].max()}\")\n",
    "    df[col] = winsorize(df[col], limits=[0.01, 0.01])\n",
    "    print(f\"After winsorizing  {col}: min={df[col].min()}, max={df[col].max()}, skew={df[col].skew():.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8eff3",
   "metadata": {},
   "source": [
    "#### Categorizing shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8295c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation of problem into a classification --> outliers caused issues for articles with low shares in regression models\n",
    "\n",
    "# explore shape of shares\n",
    "print(\"Shares value counts:\\n\", df['shares'].value_counts())\n",
    "\n",
    "# see box plot\n",
    "sns.boxplot(x = df['shares'], color = 'purple')\n",
    "plt.title('Shares')\n",
    "plt.show()\n",
    "\n",
    "# zoom in on lower range of shares\n",
    "sns.boxplot(x = df[df['shares'] < 1000]['shares'], color = 'purple')\n",
    "plt.title('Shares (Zoomed In)')\n",
    "\n",
    "# print quartile ranges\n",
    "q1, q2, q3 = np.percentile(df['shares'], [25, 50, 75])\n",
    "print(f\"Q1: {q1}, Q2: {q2}, Q3: {q3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffaf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quartiles\n",
    "q1, q2, q3 = np.percentile(df['shares'], [25, 50, 75])\n",
    "\n",
    "# assign 3 categories\n",
    "def categorize(x):\n",
    "    if x <= q1:\n",
    "        return \"Low\"\n",
    "    elif x <= q3:\n",
    "        return \"Average\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "df['shares_cat'] = df['shares'].apply(categorize)\n",
    "\n",
    "# encode\n",
    "le = LabelEncoder()\n",
    "df['y'] = le.fit_transform(df['shares_cat']) # set target equal to the categorized shares\n",
    "\n",
    "# features\n",
    "X = df.drop(columns=['shares','shares_cat','y'])\n",
    "y = df['y']\n",
    "\n",
    "# print number of instances in each category\n",
    "print(\"Category distribution:\\n\", df['shares_cat'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d8425",
   "metadata": {},
   "source": [
    "#### Anomaly detection (pt 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_anomalies_2 = []\n",
    "\n",
    "# for each of the remaining columns print the anomalies and see if there needs to be any adjustments made --> generally high rates of anomalies\n",
    "for col in cols_remaining:\n",
    "    num = anomaly_detection(col)\n",
    "    num_anomalies_2.append(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4113e",
   "metadata": {},
   "source": [
    "#### Anomaly comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for col in cols_remaining:\n",
    "    print(f'Feature: {col} || Anomalies pre-cleaning: {num_anomalies_1[index]} || Anomalies post-cleaning: {num_anomalies_2[index]}')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931158e7-7502-4ae6-ba92-7cec30a8d848",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f17836",
   "metadata": {},
   "source": [
    "#### Create Training Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff5d73-c09d-415c-8f48-fcc1a621badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTENC(categorical_features=[X.columns.get_loc(c) for c in binary_cols],\n",
    "             random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9962fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale only continuous features\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols]  = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2e11f",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2af1f3-e41b-4e13-a1d5-1f5a8b567a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a logistic regression model --> basic model for classification\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train)\n",
    "y_predict_log = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c46aa-f004-49fc-92c0-0823c5598543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate linear regression performance\n",
    "labels = [\"Low\", \"Average\", \"High\"]\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_predict_log, target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict_log)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc3ae9d",
   "metadata": {},
   "source": [
    "#### 2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor  = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "y_test_tensor  = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 51),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(51, 26),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(26, num_classes)  # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationNN(input_dim=X_train_tensor.shape[1], num_classes=3)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Test loss\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            running_test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    y_pred = torch.argmax(outputs, dim=1)\n",
    "\n",
    "labels = [\"Low\", \"Average\", \"High\"]\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test_tensor.numpy(), y_pred.numpy(), target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor.numpy(), y_pred.numpy())\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Training vs Test Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIS4930-ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
