{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b388891",
   "metadata": {},
   "source": [
    "# CIS4930 -- Final Project\n",
    "## Developed by: Chloe Fandino (Team Leader), Ashley James, Madelyne Wirbel, Chloe Nolan, Christopher Enlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81630e6b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3080896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports here :)\n",
    "\n",
    "# TODO: DELETE ---> any imports that don't end up getting used by the end of the project !!!!\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3635f",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity.csv') # import the data from the csv file --> convert to df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4edfe",
   "metadata": {},
   "source": [
    "### Exploration of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 100) # for purposes of looking at data --> need to see all rows\n",
    "\n",
    "# basic intitial looks at the dataset\n",
    "print(df.shape)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace from column names\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27bb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist()) # print out all of the available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # null values? --> NONE :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25398568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() # duplicate values? --> NONE :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539006c-7e46-4a25-8098-44037eeeaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any infinities exist in the dataframe\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "has_inf = np.isinf(numeric_df.to_numpy()).any()\n",
    "\n",
    "print(has_inf) # will need to handle in cleaning\n",
    "inf_cols = numeric_df.columns[np.isinf(numeric_df.to_numpy()).any(axis=0)].tolist()\n",
    "print(\"Columns with inf:\", inf_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8bb7e3",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76da61d",
   "metadata": {},
   "source": [
    "Drop non-predictive columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34de976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploration of potentially non-predictive features\n",
    "df['url'].nunique() == len(df) # each example has a different url --> not needed in the dataset\n",
    "df = df.drop(columns = ['url'])\n",
    "\n",
    "df = df.drop(columns = ['timedelta']) # number of days since posted until added to the dataset --> no predictive qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d588f02",
   "metadata": {},
   "source": [
    "Identify binary columns and ensure they are ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = [col for col in df.columns \n",
    "               if col.startswith(\"data_channel_is_\") or col.startswith(\"weekday_is_\")]\n",
    "\n",
    "# ensure binary indicator columns are integers\n",
    "df.columns = df.columns.str.strip()\n",
    "print(\"Binary indicator columns:\", binary_cols)\n",
    "\n",
    "for col in binary_cols:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "df[binary_cols].dtypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b6540",
   "metadata": {},
   "source": [
    "Handle inf/NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling inf\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()\n",
    "print(\"Dataset shape after removing inf/NaN:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f6d54",
   "metadata": {},
   "source": [
    "Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of feature correlation --> highly correlated features do not need to both be in dataser\n",
    "df_correlation = df.corr()\n",
    "\n",
    "sns.heatmap(df_correlation, cmap = 'coolwarm', center = 0)\n",
    "plt.title(\"Correlation Heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dafc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr = df.drop(columns=['shares'])\n",
    "y_corr = df['shares']\n",
    "\n",
    "def drop_correlated_by_importance(X, y, threshold=0.7, n_estimators=50, protected_cols=[]):\n",
    "    model = ExtraTreesRegressor(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "    model.fit(X, y)\n",
    "    importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    corr_pairs = upper.stack().reset_index()\n",
    "    corr_pairs.columns = ['feat1', 'feat2', 'corr']\n",
    "    corr_pairs = corr_pairs[corr_pairs['corr'] > threshold]\n",
    "\n",
    "    to_drop = set()\n",
    "    for _, row in corr_pairs.iterrows():\n",
    "        if row['feat1'] in protected_cols or row['feat2'] in protected_cols: # we dont want to drop binary indicator cols\n",
    "            continue\n",
    "        if importances[row['feat1']] >= importances[row['feat2']]:\n",
    "            to_drop.add(row['feat2'])\n",
    "        else:\n",
    "            to_drop.add(row['feat1'])\n",
    "    return list(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = drop_correlated_by_importance(\n",
    "    X_corr, y_corr, threshold=0.6, n_estimators=50, protected_cols=binary_cols\n",
    ")\n",
    "\n",
    "print(\"Columns to drop due to high correlation:\", columns_to_drop)\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b35de",
   "metadata": {},
   "source": [
    "### Winsorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric non-binary indicator columns\n",
    "numeric_cols = [c for c in df.columns if c not in binary_cols + ['shares']]\n",
    "\n",
    "# Winsorize\n",
    "for col in numeric_cols:\n",
    "    print(f\"Before winsorizing {col}: min={df[col].min()}, max={df[col].max()}\")\n",
    "    df[col] = winsorize(df[col], limits=[0.01, 0.01])\n",
    "    print(f\"After winsorizing {col}: min={df[col].min()}, max={df[col].max()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function with anomaly detection process --> first visualize, then calculate IQR\n",
    "\n",
    "## Currently not used in the final cleaning process, but kept for potential future use ##\n",
    "\n",
    "def anomaly_detection(feature):\n",
    "    # first boxplot to see potential outliers\n",
    "    sns.boxplot(x = df[feature], color = 'purple')\n",
    "    plt.title(feature)\n",
    "    plt.show()\n",
    "\n",
    "    # second calculate outliers based on IQR\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "\n",
    "    anomalies = df[(df[feature] < lower) | (df[feature] > upper)]\n",
    "    print('Anomalies: \\n', anomalies) # prints a list of potential anomalies\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931158e7-7502-4ae6-ba92-7cec30a8d848",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f17836",
   "metadata": {},
   "source": [
    "### Create Training Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff5d73-c09d-415c-8f48-fcc1a621badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features\n",
    "y = df[\"shares\"]\n",
    "X = df.drop(columns = [\"shares\"])\n",
    "\n",
    "print(y.value_counts())\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205870f-b5fd-421e-af97-ed971e7c0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set:\", X_train.shape)\n",
    "print(\"Test set:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc5756-c699-4bc1-8204-0ab84a573732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Skewness before log-transform:\", pd.Series(y_train).skew())\n",
    "\n",
    "y_train_knn = y_train\n",
    "y_test_knn = y_test\n",
    "y_train = np.log1p(y_train)\n",
    "y_test = np.log1p(y_test)\n",
    "\n",
    "# Check skewness after log-transform\n",
    "print(\"Skewness after log-transform:\", pd.Series(y_train).skew())\n",
    "\n",
    "sns.histplot(y_train, kde=True)\n",
    "plt.title(\"Log-transformed target distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2e11f",
   "metadata": {},
   "source": [
    "### 1. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2af1f3-e41b-4e13-a1d5-1f5a8b567a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "y_pred = lin_reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c46aa-f004-49fc-92c0-0823c5598543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate linear regression performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc3ae9d",
   "metadata": {},
   "source": [
    "### 2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features + targets to tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor  = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  # already log-scaled\n",
    "y_test_tensor  = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)   # already log-scaled\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_tensor, y_train_tensor),\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test_tensor, y_test_tensor),\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# model definition\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = RegressionNN(input_dim=X_train_tensor.shape[1])\n",
    "\n",
    "criterion = nn.MSELoss()  # MSE in log-space\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # slower lr for stability\n",
    "n_epochs = 300\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    # training step\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    train_epoch_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "\n",
    "    # test step\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            test_loss = criterion(y_pred, y_batch)\n",
    "            running_test_loss += test_loss.item() * X_batch.size(0)\n",
    "\n",
    "    test_epoch_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(test_epoch_loss)\n",
    "\n",
    "    # print every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] \"\n",
    "              f\"Train Loss: {train_epoch_loss:.4f} | \"\n",
    "              f\"Test Loss: {test_epoch_loss:.4f}\")\n",
    "\n",
    "# plot loss curves\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label=\"Train Loss (log-space)\")\n",
    "plt.plot(test_losses, label=\"Test Loss (log-space)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE (log-space)\")\n",
    "plt.title(\"Training vs Test Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# evaluation on original scale\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_log = model(X_test_tensor).numpy()\n",
    "    y_pred = np.expm1(y_pred_log).reshape(-1)             # invert log\n",
    "    y_true = np.expm1(y_test_tensor.numpy()).reshape(-1)   # invert log\n",
    "\n",
    "print(\"\\nFINAL RESULTS:\")\n",
    "print(\"Test MSE:\", mean_squared_error(y_true, y_pred))\n",
    "print(\"Test R2: \", r2_score(y_true, y_pred))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2f51c7",
   "metadata": {},
   "source": [
    "### 3. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 40)\n",
    "mse_scores = []\n",
    "\n",
    "print(\"Finding optimal K using 'distance' weights...\")\n",
    "\n",
    "for k in k_values:\n",
    "    # CRITICAL: We use weights='distance' inside the loop too\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, weights='distance')\n",
    "    knn.fit(X_train_scaled, y_train_knn.values.ravel())\n",
    "    \n",
    "    # Predict on test set\n",
    "    pred = knn.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate MSE (on raw values now)\n",
    "    mse = mean_squared_error(y_test.values, pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(k_values, mse_scores, marker='o', linestyle='dashed')\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\"MSE (Lower is Better)\")\n",
    "plt.title(\"Finding Optimal K (Distance Weighted)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Output the best K automatically\n",
    "best_k = k_values[np.argmin(mse_scores)]\n",
    "print(f\"Best K found: {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# 1. Define the model using the Best K found above\n",
    "# weights='distance' is REQUIRED to predict high values in your dataset\n",
    "knn_best = KNeighborsRegressor(n_neighbors=best_k, weights='distance')\n",
    "\n",
    "# 2. Fit the model\n",
    "knn_best.fit(X_train_scaled, y_train_knn.values.ravel()) \n",
    "\n",
    "# 3. Prediction (Directly in raw scale)\n",
    "y_pred = knn_best.predict(X_test_scaled)\n",
    "y_true = y_test_knn.values.ravel()\n",
    "\n",
    "# 4. Evaluation\n",
    "print(f\"--- FINAL RESULTS (K={best_k}, Distance Weighted) ---\")\n",
    "print(\"Max Actual Value:   \", y_true.max())\n",
    "print(\"Max Predicted Value:\", y_pred.max()) # This should now be much closer to the max actual\n",
    "print(\"-\" * 30)\n",
    "print(\"Test MSE:\", mean_squared_error(y_true, y_pred))\n",
    "print(\"Test R2: \", r2_score(y_true, y_pred))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ff66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(x=y_true, y=y_pred, alpha=0.6, color='blue', edgecolor='k', label='Predictions')\n",
    "\n",
    "# Perfect fit line\n",
    "min_val = min(y_true.min(), y_pred.min())\n",
    "max_val = max(y_true.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', lw=2, label='Perfect Fit')\n",
    "\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(f\"KNN Regression Results (k={best_k}): Predicted vs. Actual\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which feature to visualize (0 is the first column)\n",
    "feature_index = 0 \n",
    "feature_name = \"Feature 0\" # Rename this to \"Square Footage\" or \"Income\" etc. if you know it\n",
    "\n",
    "# Sort data for clean plotting\n",
    "sort_idx = X_test_scaled[:, feature_index].argsort()\n",
    "X_sorted = X_test_scaled[sort_idx]\n",
    "y_pred_sorted = y_pred[sort_idx]\n",
    "y_true_sorted = y_true[sort_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 1. Plot the actual real data (Grey dots)\n",
    "plt.scatter(X_sorted[:, feature_index], y_true_sorted, color='gray', alpha=0.4, label='Actual Data')\n",
    "\n",
    "# 2. Plot the KNN prediction (Red line)\n",
    "plt.plot(X_sorted[:, feature_index], y_pred_sorted, color='red', linewidth=2, label=f'KNN Prediction (k={best_k})')\n",
    "\n",
    "plt.xlabel(f\"Normalized {feature_name}\")\n",
    "plt.ylabel(\"Target Value\")\n",
    "plt.title(f\"KNN Model Fit on {feature_name}\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- SANITY CHECK ---\")\n",
    "print(f\"Sample Actual (y_true): {y_true[:5]}\")\n",
    "print(f\"Sample Predicted (y_pred): {y_pred[:5]}\")\n",
    "print(f\"Max Actual: {y_true.max()}\")\n",
    "print(f\"Max Predicted: {y_pred.max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
