{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b388891",
   "metadata": {},
   "source": [
    "# CIS4930 -- Final Project\n",
    "## Developed by: Chloe Fandino (Team Leader), Ashley James, Madelyne Wirbel, Chloe Nolan, Christopher Enlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961bb43",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81630e6b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3080896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports here :)\n",
    "\n",
    "# TODO: DELETE ---> any imports that don't end up getting used by the end of the project !!!!\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3635f",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity.csv') # import the data from the csv file --> convert to df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4edfe",
   "metadata": {},
   "source": [
    "### Exploration of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 100) # for purposes of looking at data --> need to see all rows\n",
    "\n",
    "# basic intitial looks at the dataset\n",
    "print(df.shape)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace from column names\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27bb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist()) # print out all of the available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # null values? --> NONE :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25398568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() # duplicate values? --> NONE :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539006c-7e46-4a25-8098-44037eeeaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any infinities exist in the dataframe\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "has_inf = np.isinf(numeric_df.to_numpy()).any()\n",
    "\n",
    "print(has_inf) # will need to handle in cleaning\n",
    "inf_cols = numeric_df.columns[np.isinf(numeric_df.to_numpy()).any(axis=0)].tolist()\n",
    "print(\"Columns with inf:\", inf_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a799c4bc",
   "metadata": {},
   "source": [
    "#### Visualization of the target variable --> shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of shares\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df[\"shares\"], bins=50)\n",
    "plt.title(\"Distribution of Shares (Raw Scale)\")\n",
    "plt.xlabel(\"Shares\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.yscale(\"log\")  # long tail\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of raw shares --> view of the outliers\n",
    "sns.boxplot(x = df[\"shares\"])\n",
    "plt.title(\"Boxplot of Shares (Visualization of Outliers)\")\n",
    "plt.ylabel(\"Shares\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9071771",
   "metadata": {},
   "source": [
    "#### Visualizations of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63373d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of key numerical features\n",
    "key_cols = [\"n_tokens_title\", \"n_tokens_content\", \"num_imgs\", \"num_hrefs\"]\n",
    "\n",
    "for col in key_cols:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(df[col], bins=50)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8bb7e3",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34de976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # exploration of potentially non-predictive features\n",
    "df = df.drop(columns=['url', 'timedelta'], errors='ignore') # url is a title and number of days since posted until added to the dataset --> no predictive qualities\n",
    "\n",
    "# feature engineering\n",
    "df['rate_positive_words'] = df['global_rate_positive_words'] / (df['n_tokens_content'] + 1)\n",
    "df['rate_negative_words'] = df['global_rate_negative_words'] / (df['n_tokens_content'] + 1)\n",
    "df['emotional_polarity'] = df['global_sentiment_polarity'].abs()\n",
    "df['title_body_sentiment_ratio'] = df['title_sentiment_polarity'] / (df['global_sentiment_polarity'] + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in two --> based on median\n",
    "median_shares = df['shares'].median()\n",
    "print(f\"Splitting data at median shares: {median_shares}\")\n",
    "\n",
    "def categorize(x):\n",
    "    return 1 if x > median_shares else 0\n",
    "\n",
    "df['y'] = df['shares'].apply(categorize)\n",
    "\n",
    "# define X and y \n",
    "X = df.drop(columns=['shares', 'y'])\n",
    "y = df['y']\n",
    "\n",
    "binary_cols = [col for col in X.columns if \"data_channel\" in col or \"weekday\" in col or \"is_weekend\" in col]\n",
    "# ensure binary cols are actually integers\n",
    "for col in binary_cols:\n",
    "    X[col] = X[col].astype(int)\n",
    "\n",
    "numeric_cols = [col for col in X.columns if col not in binary_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> test-train-split <-- DO NOT EDIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHLOE pearson correlation coefficient\n",
    "corr_matrix = X_train[numeric_cols].corr(method='pearson').abs()\n",
    "\n",
    "# visualization of highly correlated features\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    cmap='coolwarm',\n",
    "    annot=False,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    "    square=True\n",
    ")\n",
    "plt.title(\"Pearson Correlation Heatmap of Numeric Features (absolute value)\")\n",
    "plt.show()\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "\n",
    "print(f\"Dropping {len(to_drop)} columns due to correlation: {to_drop}\")\n",
    "\n",
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)\n",
    "numeric_cols = [c for c in numeric_cols if c not in to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f9d86",
   "metadata": {},
   "source": [
    "#### Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize and count anomalies\n",
    "def anomaly_detection(feature, visualize):\n",
    "    # first boxplot to see potential outliers\n",
    "    if visualize:\n",
    "        sns.boxplot(x = df[feature], color = 'purple')\n",
    "        plt.title(feature)\n",
    "        plt.show()\n",
    "\n",
    "    # second calculate outliers based on IQR\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "\n",
    "    anomalies = df[(df[feature] < lower) | (df[feature] > upper)]\n",
    "    print('Anomalies: \\n', anomalies) # prints a list of potential anomalies\n",
    "\n",
    "    num_anomalies = anomalies.shape[0]\n",
    "    return num_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78744fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE OUTLIER HANDLING --> visualize outliers\n",
    "cols_remaining = df.columns.tolist() # what columns are left in the dataset\n",
    "\n",
    "num_anomalies_1 = []\n",
    "\n",
    "# for each of the remaining columns print the anomalies and see if there needs to be any adjustments made --> generally high rates of anomalies\n",
    "for col in cols_remaining:\n",
    "    num = anomaly_detection(col, True)\n",
    "    num_anomalies_1.append(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1b0da",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad42246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHLOE handling skew and outliers\n",
    "scaler = RobustScaler()\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfa28d",
   "metadata": {},
   "source": [
    "#### Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402bb39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHLOE TRAIN SMOTE NAN\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "#CLASS BALANCING (SMOTE)\n",
    "cat_indices = [X_train.columns.get_loc(c) for c in binary_cols if c in X_train.columns]\n",
    "sm = SMOTENC(categorical_features=cat_indices, random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccfad7",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHLOE FEATURE SELECTION\n",
    "cat_indices = [X_train.columns.get_loc(c) for c in binary_cols if c in X_train.columns]\n",
    "selector_model = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)\n",
    "selector_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "selection = SelectFromModel(selector_model, threshold=\"1.25*median\", prefit=True)\n",
    "X_train_selected = selection.transform(X_train_res)\n",
    "X_test_selected = selection.transform(X_test)\n",
    "\n",
    "selected_features_names = X_train_res.columns[selection.get_support()]\n",
    "print(f\"Selected {len(selected_features_names)} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931158e7-7502-4ae6-ba92-7cec30a8d848",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fcecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to declare the variable once\n",
    "labels = ['Not Viral', 'Viral']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2e11f",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2af1f3-e41b-4e13-a1d5-1f5a8b567a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a logistic regression model --> basic model for classification\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train)\n",
    "y_predict_log = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c46aa-f004-49fc-92c0-0823c5598543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate linear regression performance\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_predict_log, target_names = labels))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict_log)\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Purples', xticklabels = labels, yticklabels = labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b934d",
   "metadata": {},
   "source": [
    "#### 2. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6925d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal value of k\n",
    "k_values = range(1, 40)\n",
    "accuracy_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Build KNN classifier for each k\n",
    "    knn = KNeighborsClassifier(n_neighbors = k, weights = 'uniform')\n",
    "    knn.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # Predict on test set\n",
    "    pred = knn.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_test.ravel(), pred)\n",
    "    accuracy_scores.append(acc)\n",
    "\n",
    "plt.plot(k_values, accuracy_scores, marker='o', linestyle='dashed', color='green')\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Finding Optimal K for Binary Classification\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[np.argmax(accuracy_scores)]\n",
    "print(f\"Best K found: {best_k} with Accuracy: {max(accuracy_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ffb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train knn with optimal k\n",
    "knn_best = KNeighborsClassifier(n_neighbors=best_k, weights='uniform')\n",
    "\n",
    "# Fit the model\n",
    "knn_best.fit(X_train, y_train.values.ravel()) \n",
    "\n",
    "# Prediction\n",
    "y_pred_knn = knn_best.predict(X_test)\n",
    "\n",
    "# visualize success of knn\n",
    "cm = confusion_matrix(y_test, y_pred_knn)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"KNN Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "print(f\"--- FINAL KNN RESULTS (K={best_k}) ---\")\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test.values.ravel(), y_pred_knn, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b624315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample to prevent kernel crash\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "X_vis = X_train.sample(3000, random_state=42)  \n",
    "y_vis = y_train.loc[X_vis.index]\n",
    "\n",
    "# PCA on smaller data\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_vis)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train KNN on subsample\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X_train_pca, y_vis.values.ravel())\n",
    "\n",
    "h = 1\n",
    "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, h),\n",
    "    np.arange(y_min, y_max, h)\n",
    ")\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "cmap_light = ListedColormap(['#FFCCCC', '#CCCCFF'])\n",
    "cmap_bold  = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.3)\n",
    "plt.scatter(\n",
    "    X_test_pca[:, 0],\n",
    "    X_test_pca[:, 1],\n",
    "    c=y_test.values.ravel(),\n",
    "    cmap=cmap_bold,\n",
    "    edgecolor='k',\n",
    "    s=30,\n",
    "    alpha=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df76823",
   "metadata": {},
   "source": [
    "#### 3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHLOE RANDOM FOREST\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=20, \n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_predict_rf = rf_model.predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"\\nRandom Forest Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_predict_rf, target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict_rf)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.show()\n",
    "#feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_df = feature_df.sort_values(by='Importance', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the trained model\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to organize them\n",
    "# Assuming X_train was a DataFrame originally. If it was a numpy array, \n",
    "# we generate generic names Feature_0, Feature_1, etc.\n",
    "try:\n",
    "    feature_names = X_train.columns\n",
    "except:\n",
    "    feature_names = [f\"Feature_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot Top 20 Features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_imp_df.head(20), palette='viridis')\n",
    "plt.title(\"Top 20 Features driving the Random Forest\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "print(feature_imp_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59daf3c5",
   "metadata": {},
   "source": [
    "#### 4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49463b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    objective='binary:logistic'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_selected, y_train_res)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test_selected)\n",
    "\n",
    "# evaluate\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"XGBoost Binary Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1ca90",
   "metadata": {},
   "source": [
    "#### 5. Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95223f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: something here plz\n",
    "\n",
    "# define the learners\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "]\n",
    "\n",
    "# stacking model\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_res.values.ravel())\n",
    "\n",
    "y_pred_stack = stacking_model.predict(X_test_selected)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test.values.ravel(), y_pred_stack, target_names=labels))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm_stack = confusion_matrix(y_test.values.ravel(), y_pred_stack)\n",
    "sns.heatmap(cm_stack, annot=True, fmt='d', cmap='Purples', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Stacking Model Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee8f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probability_stacking = stacking_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "fpr_stack, tpr_stack, thresholds_stack = roc_curve(y_test.values.ravel(), y_probability_stacking)\n",
    "\n",
    "roc_auc_stack = auc(fpr_stack, tpr_stack)\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_stack, tpr_stack, color='purple', lw=2, label=f'Stacking AUC = {roc_auc_stack:.4f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Stacking Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20156b14",
   "metadata": {},
   "source": [
    "#### 6. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: we still need this pls do this\n",
    "\n",
    "# using linear svc to save time, very similar results\n",
    "svm_model = LinearSVC(random_state=42)\n",
    "\n",
    "svm_model.fit(X_train_selected, y_train_res.values.ravel())\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test_selected)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test.values.ravel(), y_pred_svm, target_names=labels))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm_svm = confusion_matrix(y_test.values.ravel(), y_pred_svm)\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Oranges', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"SVM Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probability_svm = svm_model.decision_function(X_test_selected)\n",
    "\n",
    "fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test.values.ravel(), y_probability_svm)\n",
    "\n",
    "roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_svm, tpr_svm, color='darkorange', lw=2, label=f'SVM AUC = {roc_auc_svm:.4f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Support Vector Machine (SVM)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc3ae9d",
   "metadata": {},
   "source": [
    "#### 7. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor  = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "y_test_tensor  = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: investigate the architecture of the NN --> increase the complexity to better learn the pattern of the data\n",
    "class ClassificationNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 51),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(51, 26), # hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(26, num_classes)  # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationNN(input_dim=X_train_tensor.shape[1], num_classes=3)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Test loss\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            running_test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "#evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    y_pred = torch.argmax(outputs, dim=1)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test_tensor.numpy(), y_pred.numpy(), target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor.numpy(), y_pred.numpy())\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Training vs Test Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
